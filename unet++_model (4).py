# -*- coding: utf-8 -*-
"""unet++ model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n9oAbXdmSO9H5dfGQA1pymZQZTIW-4S4

# rectus femoris muscle
"""

# Step 1: Mount Google Drive and Install Libraries

# --- 1.1: Mount Google Drive ---
# This will prompt you for authorization.
from google.colab import drive
drive.mount('/content/drive')

# --- 1.2: Install Segmentation Models Pytorch library ---
# This library contains many pre-built models like U-Net, U-Net++, Attention U-Net, etc.
!pip install segmentation-models-pytorch -q

# --- 1.3: Import all necessary libraries ---
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
from PIL import Image
import random
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

# Imports for the model and transformations
import segmentation_models_pytorch as smp
import torchvision.transforms.functional as F
from torchvision import transforms
from sklearn.model_selection import train_test_split

print("Libraries installed and imported successfully!")

# ========================================================================
# This is the simplified Dataset class without transformations (CORRECTED)
# ========================================================================
class UltrasoundNpyDataset_NoTransforms(Dataset):
    def __init__(self, x_data, y_data):
        self.x_data = x_data
        self.y_data = y_data

    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, idx):
        image_np = self.x_data[idx]
        mask_np = self.y_data[idx]

        image_tensor = torch.from_numpy(image_np).float()
        if image_tensor.ndim == 3 and image_tensor.shape[-1] in [1, 3]:
            image_tensor = image_tensor.permute(2, 0, 1)
        elif image_tensor.ndim == 2:
            image_tensor = image_tensor.unsqueeze(0)

        if mask_np.ndim == 3 and mask_np.shape[-1] == 1:
            mask_np = np.squeeze(mask_np, axis=-1)
        mask_tensor = torch.from_numpy(mask_np).long()
        mask_tensor = mask_tensor.unsqueeze(0)

        return image_tensor, mask_tensor


# ========================================================================
# The rest of your script, now using the new Dataset class
# ========================================================================

# --- Step 1: Define File Paths ---
# This path points to the folder containing your data.
DATA_FOLDER = '/content/drive/MyDrive/intern RF transverse latest file/'

X_TRAIN_PATH = os.path.join(DATA_FOLDER, 'X_train.npy')
Y_TRAIN_PATH = os.path.join(DATA_FOLDER, 'y_train.npy')
X_VAL_PATH = os.path.join(DATA_FOLDER, 'X_val.npy')
Y_VAL_PATH = os.path.join(DATA_FOLDER, 'y_val.npy')
X_TEST_PATH = os.path.join(DATA_FOLDER, 'X_test.npy')
Y_TEST_PATH = os.path.join(DATA_FOLDER, 'y_test.npy')


# --- Step 2: Load Data and Create DataLoaders ---
print("Loading pre-split data...")
x_train = np.load(X_TRAIN_PATH)
y_train = np.load(Y_TRAIN_PATH)
x_val = np.load(X_VAL_PATH)
y_val = np.load(Y_VAL_PATH)
x_test = np.load(X_TEST_PATH)
y_test = np.load(Y_TEST_PATH)
print("Data loaded successfully.")

# Create Dataset instances USING THE NEW CLASS
train_dataset = UltrasoundNpyDataset_NoTransforms(x_train, y_train)
val_dataset = UltrasoundNpyDataset_NoTransforms(x_val, y_val)
test_dataset = UltrasoundNpyDataset_NoTransforms(x_test, y_test)

# Create DataLoader instances
BATCH_SIZE = 8
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

print(f"\nCreated {len(train_dataset)} training examples (without transformations).")
print(f"Created {len(val_dataset)} validation examples (without transformations).")
print(f"Created {len(test_dataset)} testing examples (without transformations).")
print("Data preparation complete.")

# Step 3: Define the U-Net++ Model

# --- 3.1: Set up the device (GPU or CPU) ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")

# --- 3.2: Create the U-Net++ Model ---
# The only change is calling smp.UnetPlusPlus instead of smp.Unet.
# The rest of the parameters (backbone, weights, etc.) remain the same.
model = smp.UnetPlusPlus(
    encoder_name="resnet34",        # You can use the same backbone
    encoder_weights="imagenet",     # Continue using pre-trained weights
    in_channels=1,                  # Input channels
    classes=1,                      # Output classes (binary mask)
)

model.to(DEVICE)

print("U-Net++ model created successfully.")
# You can uncomment the line below to see the more complex architecture
# print(model)

# Step 4: Configure Training Components

# --- 4.1: Loss Function ---
loss_fn = nn.BCEWithLogitsLoss()

# --- 4.2: Optimizer ---
LEARNING_RATE = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

# --- 4.3: Metrics Calculation (Dice Score) ---
def dice_score(preds, targets, smooth=1e-6):
    preds = torch.sigmoid(preds)
    preds = (preds > 0.5).float()
    preds = preds.view(-1)
    targets = targets.view(-1)
    intersection = (preds * targets).sum()
    dice = (2. * intersection + smooth) / (preds.sum() + targets.sum() + smooth)
    return dice

print("Loss function, optimizer, and metrics are configured.")

# Step 5: The Training and Validation Loop

NUM_EPOCHS = 50

# --- Initialize variables to track the best model ---
best_val_dice = 0.0  # Start with a low value for Dice score
MODEL_SAVE_PATH = '/content/drive/MyDrive/internship models/unet++ model/unet++_resnet34_best.pth'

# --- 5.1: Training Loop ---
for epoch in range(NUM_EPOCHS):
    print(f"--- Epoch {epoch+1}/{NUM_EPOCHS} ---")

    # --- Training Phase ---
    model.train()
    train_loss = 0.0
    train_dice = 0.0
    for images, masks in tqdm(train_loader, desc="Training"):
        images = images.to(DEVICE)
        masks = masks.float().to(DEVICE)
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        train_dice += dice_score(outputs, masks).item()

    avg_train_loss = train_loss / len(train_loader)
    avg_train_dice = train_dice / len(train_loader)

    # --- Validation Phase ---
    model.eval()
    val_loss = 0.0
    val_dice = 0.0
    with torch.no_grad():
        for images, masks in tqdm(val_loader, desc="Validation"):
            images = images.to(DEVICE)
            masks = masks.float().to(DEVICE)
            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item()
            val_dice += dice_score(outputs, masks).item()

    avg_val_loss = val_loss / len(val_loader)
    avg_val_dice = val_dice / len(val_loader)

    # --- Check if this is the best model based on validation Dice score ---
    if avg_val_dice > best_val_dice:
        best_val_dice = avg_val_dice
        # Save the model's state dictionary
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(f"New best model saved at epoch {epoch+1} with Val Dice: {avg_val_dice:.4f} to {MODEL_SAVE_PATH}")

    # --- Print Epoch Results ---
    print(f"Epoch {epoch+1} Summary:")
    print(f"  Train Loss: {avg_train_loss:.4f}, Train Dice: {avg_train_dice:.4f}")
    print(f"  Val Loss:   {avg_val_loss:.4f}, Val Dice:   {avg_val_dice:.4f}\n")

print("Training finished!")
print(f"Best model was saved with Validation Dice: {best_val_dice:.4f} to {MODEL_SAVE_PATH}")

# New step 6
# --- 6.1: Import necessary libraries ---
import os
from scipy import ndimage
import matplotlib.pyplot as plt
import numpy as np
import torch
from tqdm import tqdm
import segmentation_models_pytorch as smp

# --- 6.2: Define the Post-Processing Function (Unchanged) ---
def post_process_mask(mask):
    """
    Applies post-processing to a binary segmentation mask.
    This function finds all disconnected objects (connected components) and keeps only the largest one,
    then fills any holes within that largest object.
    """
    labels, num_features = ndimage.label(mask)
    if num_features == 0:
        return mask

    component_sizes = np.bincount(labels.ravel())
    if len(component_sizes) > 1:
        largest_component_label = component_sizes[1:].argmax() + 1
        processed_mask = (labels == largest_component_label)
        processed_mask = ndimage.binary_fill_holes(processed_mask)
        return processed_mask.astype(np.uint8)
    else:
        return mask

# --- 6.3: Load the Best Model (Updated) ---
print("Loading the best model for evaluation...")
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_model = smp.UnetPlusPlus(
    encoder_name="resnet34",
    encoder_weights=None,  # Weights are loaded from file, not pre-trained
    in_channels=1,         # Changed to 1 to match the trained model
    classes=1,
)
MODEL_SAVE_PATH = '/content/drive/MyDrive/internship models/unet++ model/rectus femoris/unet++_resnet34_best.pth'
eval_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=torch.device(DEVICE)))
eval_model.to(DEVICE)
eval_model.eval()

# --- 6.4: Define Save Directories and Visualization Helper ---
BASE_SAVE_DIR = '/content/drive/MyDrive/internship models/unet++ model/rectus femoris/segmentation_results_with_preprocessing'
TRAIN_SAVE_DIR = os.path.join(BASE_SAVE_DIR, 'train_set_predictions')
TEST_SAVE_DIR = os.path.join(BASE_SAVE_DIR, 'test_set_predictions')

os.makedirs(TRAIN_SAVE_DIR, exist_ok=True)
os.makedirs(TEST_SAVE_DIR, exist_ok=True)

print(f"Train set predictions will be saved to: {TRAIN_SAVE_DIR}")
print(f"Test set predictions will be saved to: {TEST_SAVE_DIR}")

def visualize_and_save(processed_img, gt_mask, pred_raw, pred_post, save_path, title):
    """Helper function to plot and save comparison images."""
    if processed_img.shape[0] == 1:  # Grayscale
        processed_img_display = processed_img.cpu().squeeze(0).numpy()
        processed_img_display = (processed_img_display - processed_img_display.min()) / (processed_img_display.max() - processed_img_display.min())
    else:  # RGB (repeated channel) - unlikely now with in_channels=1
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        processed_img_display = processed_img.cpu().permute(1, 2, 0).numpy()
        processed_img_display = std * processed_img_display + mean
        processed_img_display = np.clip(processed_img_display, 0, 1)

    fig, axes = plt.subplots(1, 4, figsize=(20, 5))
    axes[0].imshow(processed_img_display, cmap='gray')  # Force grayscale for in_channels=1
    axes[0].set_title("Input Image"); axes[0].axis('off')
    axes[1].imshow(np.squeeze(gt_mask), cmap='gray'); axes[1].set_title("Ground Truth"); axes[1].axis('off')
    axes[2].imshow(np.squeeze(pred_raw), cmap='gray'); axes[2].set_title("Raw Prediction"); axes[2].axis('off')
    axes[3].imshow(np.squeeze(pred_post), cmap='gray'); axes[3].set_title("Post-Processed"); axes[3].axis('off')
    plt.suptitle(title, fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close(fig)  # Close the figure to save memory

# --- 6.5: Run Evaluation on the ENTIRE Test Set & Save ALL Predictions ---
print("\n--- Evaluating ENTIRE Test Set and Saving All Predictions ---")
total_dice_before = 0
total_dice_after = 0
num_samples_test = 0
smooth = 1e-6

with torch.no_grad():
    for i, (images, gt_masks) in enumerate(tqdm(test_loader, desc="Testing and Saving")):
        images = images.to(DEVICE)
        gt_masks_np = gt_masks.cpu().numpy()

        preds_logits = eval_model(images)
        preds_sigmoid = torch.sigmoid(preds_logits)
        preds_before_np = (preds_sigmoid > 0.5).cpu().numpy()
        preds_after_np = np.array([post_process_mask(np.squeeze(p)) for p in preds_before_np])

        # Loop through each image in the batch to save it
        for j in range(images.shape[0]):
            image_idx = i * test_loader.batch_size + j

            gt = np.squeeze(gt_masks_np[j]).flatten()
            pred_before = np.squeeze(preds_before_np[j]).flatten()
            pred_after = np.squeeze(preds_after_np[j]).flatten()

            intersection_before = (pred_before * gt).sum()
            total_dice_before += (2. * intersection_before + smooth) / (pred_before.sum() + gt.sum() + smooth)

            intersection_after = (pred_after * gt).sum()
            total_dice_after += (2. * intersection_after + smooth) / (pred_after.sum() + gt.sum() + smooth)

            num_samples_test += 1

            # Define the save path for this specific image
            save_path = os.path.join(TEST_SAVE_DIR, f"test_prediction_{image_idx+1}.png")

            # Call the helper function to save the plot
            visualize_and_save(
                processed_img=images[j],
                gt_mask=gt_masks_np[j],
                pred_raw=preds_before_np[j],
                pred_post=preds_after_np[j],
                save_path=save_path,
                title=f"Test Set - Prediction {image_idx+1}"
            )

# --- 6.6: Print Final Test Results ---
avg_dice_before_test = total_dice_before / num_samples_test
avg_dice_after_test = total_dice_after / num_samples_test
print(f"\n--- Test Set Evaluation Complete ---")
print(f"Total Test Images Processed: {num_samples_test}")
print(f"Average Dice (Before Post-Processing): {avg_dice_before_test:.4f}")
print(f"Average Dice (After Post-Processing):  {avg_dice_after_test:.4f}")

# --- 6.7: Run Prediction on the ENTIRE Train Set & Save ALL Predictions ---
print("\n--- Evaluating ENTIRE Train Set and Saving All Predictions ---")
num_samples_train = 0
total_dice_before_train = 0
total_dice_after_train = 0

with torch.no_grad():
    for i, (images, gt_masks) in enumerate(tqdm(train_loader, desc="Training Set Prediction")):
        images = images.to(DEVICE)
        gt_masks_np = gt_masks.cpu().numpy()

        preds_logits = eval_model(images)
        preds_sigmoid = torch.sigmoid(preds_logits)
        preds_before_np = (preds_sigmoid > 0.5).cpu().numpy()
        preds_after_np = np.array([post_process_mask(np.squeeze(p)) for p in preds_before_np])

        # Loop through each image in the batch to save it
        for j in range(images.shape[0]):
            image_idx = i * train_loader.batch_size + j

            gt = np.squeeze(gt_masks_np[j]).flatten()
            pred_before = np.squeeze(preds_before_np[j]).flatten()
            pred_after = np.squeeze(preds_after_np[j]).flatten()

            intersection_before = (pred_before * gt).sum()
            total_dice_before_train += (2. * intersection_before + smooth) / (pred_before.sum() + gt.sum() + smooth)

            intersection_after = (pred_after * gt).sum()
            total_dice_after_train += (2. * intersection_after + smooth) / (pred_after.sum() + gt.sum() + smooth)

            num_samples_train += 1

            # Define the save path for this specific image
            save_path = os.path.join(TRAIN_SAVE_DIR, f"train_prediction_{image_idx+1}.png")

            # Call the helper function to save the plot
            visualize_and_save(
                processed_img=images[j],
                gt_mask=gt_masks_np[j],
                pred_raw=preds_before_np[j],
                pred_post=preds_after_np[j],
                save_path=save_path,
                title=f"Train Set - Prediction {image_idx+1}"
            )

# --- 6.8: Print Final Train Results ---
avg_dice_before_train = total_dice_before_train / num_samples_train
avg_dice_after_train = total_dice_after_train / num_samples_train
print(f"\n--- Train Set Evaluation Complete ---")
print(f"Total Train Images Processed: {num_samples_train}")
print(f"Average Dice (Before Post-Processing): {avg_dice_before_train:.4f}")
print(f"Average Dice (After Post-Processing):  {avg_dice_after_train:.4f}")
print(f"All predictions saved successfully to Google Drive.")

"""# vastus medialis msucle

"""

# Step 1: Mount Google Drive and Install Libraries

# --- 1.1: Mount Google Drive ---
# This will prompt you for authorization.
from google.colab import drive
drive.mount('/content/drive')

# --- 1.2: Install Segmentation Models Pytorch library ---
# This library contains many pre-built models like U-Net, U-Net++, Attention U-Net, etc.
!pip install segmentation-models-pytorch -q

# --- 1.3: Import all necessary libraries ---
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
from PIL import Image
import random
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

# Imports for the model and transformations
import segmentation_models_pytorch as smp
import torchvision.transforms.functional as F
from torchvision import transforms
from sklearn.model_selection import train_test_split

print("Libraries installed and imported successfully!")

# ========================================================================
# This is the simplified Dataset class without transformations (CORRECTED)
# ========================================================================
class UltrasoundNpyDataset_NoTransforms(Dataset):
    def __init__(self, x_data, y_data):
        self.x_data = x_data
        self.y_data = y_data

    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, idx):
        image_np = self.x_data[idx]
        mask_np = self.y_data[idx]

        image_tensor = torch.from_numpy(image_np).float()
        if image_tensor.ndim == 3 and image_tensor.shape[-1] in [1, 3]:
            image_tensor = image_tensor.permute(2, 0, 1)
        elif image_tensor.ndim == 2:
            image_tensor = image_tensor.unsqueeze(0)

        if mask_np.ndim == 3 and mask_np.shape[-1] == 1:
            mask_np = np.squeeze(mask_np, axis=-1)
        mask_tensor = torch.from_numpy(mask_np).long()
        mask_tensor = mask_tensor.unsqueeze(0)

        return image_tensor, mask_tensor

# --- Step 1: Define File Paths ---
# This path points to the folder containing your data.
DATA_FOLDER = '/content/drive/MyDrive/intern RF longitudinal latest file/'

X_TRAIN_PATH = os.path.join(DATA_FOLDER, 'X_train.npy')
Y_TRAIN_PATH = os.path.join(DATA_FOLDER, 'y_train.npy')
X_VAL_PATH = os.path.join(DATA_FOLDER, 'X_val.npy')
Y_VAL_PATH = os.path.join(DATA_FOLDER, 'y_val.npy')
X_TEST_PATH = os.path.join(DATA_FOLDER, 'X_test.npy')
Y_TEST_PATH = os.path.join(DATA_FOLDER, 'y_test.npy')


# --- Step 2: Load Data and Create DataLoaders ---
print("Loading pre-split data...")
x_train = np.load(X_TRAIN_PATH)
y_train = np.load(Y_TRAIN_PATH)
x_val = np.load(X_VAL_PATH)
y_val = np.load(Y_VAL_PATH)
x_test = np.load(X_TEST_PATH)
y_test = np.load(Y_TEST_PATH)
print("Data loaded successfully.")

# Create Dataset instances USING THE NEW CLASS
train_dataset = UltrasoundNpyDataset_NoTransforms(x_train, y_train)
val_dataset = UltrasoundNpyDataset_NoTransforms(x_val, y_val)
test_dataset = UltrasoundNpyDataset_NoTransforms(x_test, y_test)

# Create DataLoader instances
BATCH_SIZE = 8
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

print(f"\nCreated {len(train_dataset)} training examples (without transformations).")
print(f"Created {len(val_dataset)} validation examples (without transformations).")
print(f"Created {len(test_dataset)} testing examples (without transformations).")
print("Data preparation complete.")

# Step 3: Define the U-Net++ Model

# --- 3.1: Set up the device (GPU or CPU) ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")

# --- 3.2: Create the U-Net++ Model ---
# The only change is calling smp.UnetPlusPlus instead of smp.Unet.
# The rest of the parameters (backbone, weights, etc.) remain the same.
model = smp.UnetPlusPlus(
    encoder_name="resnet34",        # You can use the same backbone
    encoder_weights="imagenet",     # Continue using pre-trained weights
    in_channels=1,                  # Input channels
    classes=1,                      # Output classes (binary mask)
)

model.to(DEVICE)

print("U-Net++ model created successfully.")
# You can uncomment the line below to see the more complex architecture
# print(model)

# Step 4: Configure Training Components

# --- 4.1: Loss Function ---
loss_fn = nn.BCEWithLogitsLoss()

# --- 4.2: Optimizer ---
LEARNING_RATE = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

# --- 4.3: Metrics Calculation (Dice Score) ---
def dice_score(preds, targets, smooth=1e-6):
    preds = torch.sigmoid(preds)
    preds = (preds > 0.5).float()
    preds = preds.view(-1)
    targets = targets.view(-1)
    intersection = (preds * targets).sum()
    dice = (2. * intersection + smooth) / (preds.sum() + targets.sum() + smooth)
    return dice

print("Loss function, optimizer, and metrics are configured.")

# Step 5: The Training and Validation Loop

NUM_EPOCHS = 50

# --- Initialize variables to track the best model ---
best_val_dice = 0.0  # Start with a low value for Dice score
MODEL_SAVE_PATH = '/content/drive/MyDrive/internship models/unet++ model/vastus medialis/unet++_resnet34_best.pth'

# --- 5.1: Training Loop ---
for epoch in range(NUM_EPOCHS):
    print(f"--- Epoch {epoch+1}/{NUM_EPOCHS} ---")

    # --- Training Phase ---
    model.train()
    train_loss = 0.0
    train_dice = 0.0
    for images, masks in tqdm(train_loader, desc="Training"):
        images = images.to(DEVICE)
        masks = masks.float().to(DEVICE)
        outputs = model(images)
        loss = loss_fn(outputs, masks)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        train_dice += dice_score(outputs, masks).item()

    avg_train_loss = train_loss / len(train_loader)
    avg_train_dice = train_dice / len(train_loader)

    # --- Validation Phase ---
    model.eval()
    val_loss = 0.0
    val_dice = 0.0
    with torch.no_grad():
        for images, masks in tqdm(val_loader, desc="Validation"):
            images = images.to(DEVICE)
            masks = masks.float().to(DEVICE)
            outputs = model(images)
            loss = loss_fn(outputs, masks)
            val_loss += loss.item()
            val_dice += dice_score(outputs, masks).item()

    avg_val_loss = val_loss / len(val_loader)
    avg_val_dice = val_dice / len(val_loader)

    # --- Check if this is the best model based on validation Dice score ---
    if avg_val_dice > best_val_dice:
        best_val_dice = avg_val_dice
        # Save the model's state dictionary
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(f"New best model saved at epoch {epoch+1} with Val Dice: {avg_val_dice:.4f} to {MODEL_SAVE_PATH}")

    # --- Print Epoch Results ---
    print(f"Epoch {epoch+1} Summary:")
    print(f"  Train Loss: {avg_train_loss:.4f}, Train Dice: {avg_train_dice:.4f}")
    print(f"  Val Loss:   {avg_val_loss:.4f}, Val Dice:   {avg_val_dice:.4f}\n")

print("Training finished!")
print(f"Best model was saved with Validation Dice: {best_val_dice:.4f} to {MODEL_SAVE_PATH}")

# New step 6
# --- 6.1: Import necessary libraries ---
import os
from scipy import ndimage
import matplotlib.pyplot as plt
import numpy as np
import torch
from tqdm import tqdm
import segmentation_models_pytorch as smp

# --- 6.2: Define the Post-Processing Function (Unchanged) ---
def post_process_mask(mask):
    """
    Applies post-processing to a binary segmentation mask.
    This function finds all disconnected objects (connected components) and keeps only the largest one,
    then fills any holes within that largest object.
    """
    labels, num_features = ndimage.label(mask)
    if num_features == 0:
        return mask

    component_sizes = np.bincount(labels.ravel())
    if len(component_sizes) > 1:
        largest_component_label = component_sizes[1:].argmax() + 1
        processed_mask = (labels == largest_component_label)
        processed_mask = ndimage.binary_fill_holes(processed_mask)
        return processed_mask.astype(np.uint8)
    else:
        return mask

# --- 6.3: Load the Best Model (Updated) ---
print("Loading the best model for evaluation...")
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_model = smp.UnetPlusPlus(
    encoder_name="resnet34",
    encoder_weights=None,  # Weights are loaded from file, not pre-trained
    in_channels=1,         # Changed to 1 to match the trained model
    classes=1,
)
MODEL_SAVE_PATH = '/content/drive/MyDrive/internship models/unet++ model/vastus medialis/unet++_resnet34_best.pth'
eval_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=torch.device(DEVICE)))
eval_model.to(DEVICE)
eval_model.eval()

# --- 6.4: Define Save Directories and Visualization Helper ---
BASE_SAVE_DIR = '/content/drive/MyDrive/internship models/unet++ model/vastus medialis/segmentation_results_with_preprocessing'
TRAIN_SAVE_DIR = os.path.join(BASE_SAVE_DIR, 'train_set_predictions')
TEST_SAVE_DIR = os.path.join(BASE_SAVE_DIR, 'test_set_predictions')

os.makedirs(TRAIN_SAVE_DIR, exist_ok=True)
os.makedirs(TEST_SAVE_DIR, exist_ok=True)

print(f"Train set predictions will be saved to: {TRAIN_SAVE_DIR}")
print(f"Test set predictions will be saved to: {TEST_SAVE_DIR}")

def visualize_and_save(processed_img, gt_mask, pred_raw, pred_post, save_path, title):
    """Helper function to plot and save comparison images."""
    if processed_img.shape[0] == 1:  # Grayscale
        processed_img_display = processed_img.cpu().squeeze(0).numpy()
        processed_img_display = (processed_img_display - processed_img_display.min()) / (processed_img_display.max() - processed_img_display.min())
    else:  # RGB (repeated channel) - unlikely now with in_channels=1
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        processed_img_display = processed_img.cpu().permute(1, 2, 0).numpy()
        processed_img_display = std * processed_img_display + mean
        processed_img_display = np.clip(processed_img_display, 0, 1)

    fig, axes = plt.subplots(1, 4, figsize=(20, 5))
    axes[0].imshow(processed_img_display, cmap='gray')  # Force grayscale for in_channels=1
    axes[0].set_title("Input Image"); axes[0].axis('off')
    axes[1].imshow(np.squeeze(gt_mask), cmap='gray'); axes[1].set_title("Ground Truth"); axes[1].axis('off')
    axes[2].imshow(np.squeeze(pred_raw), cmap='gray'); axes[2].set_title("Raw Prediction"); axes[2].axis('off')
    axes[3].imshow(np.squeeze(pred_post), cmap='gray'); axes[3].set_title("Post-Processed"); axes[3].axis('off')
    plt.suptitle(title, fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close(fig)  # Close the figure to save memory

# --- 6.5: Run Evaluation on the ENTIRE Test Set & Save ALL Predictions ---
print("\n--- Evaluating ENTIRE Test Set and Saving All Predictions ---")
total_dice_before = 0
total_dice_after = 0
num_samples_test = 0
smooth = 1e-6

with torch.no_grad():
    for i, (images, gt_masks) in enumerate(tqdm(test_loader, desc="Testing and Saving")):
        images = images.to(DEVICE)
        gt_masks_np = gt_masks.cpu().numpy()

        preds_logits = eval_model(images)
        preds_sigmoid = torch.sigmoid(preds_logits)
        preds_before_np = (preds_sigmoid > 0.5).cpu().numpy()
        preds_after_np = np.array([post_process_mask(np.squeeze(p)) for p in preds_before_np])

        # Loop through each image in the batch to save it
        for j in range(images.shape[0]):
            image_idx = i * test_loader.batch_size + j

            gt = np.squeeze(gt_masks_np[j]).flatten()
            pred_before = np.squeeze(preds_before_np[j]).flatten()
            pred_after = np.squeeze(preds_after_np[j]).flatten()

            intersection_before = (pred_before * gt).sum()
            total_dice_before += (2. * intersection_before + smooth) / (pred_before.sum() + gt.sum() + smooth)

            intersection_after = (pred_after * gt).sum()
            total_dice_after += (2. * intersection_after + smooth) / (pred_after.sum() + gt.sum() + smooth)

            num_samples_test += 1

            # Define the save path for this specific image
            save_path = os.path.join(TEST_SAVE_DIR, f"test_prediction_{image_idx+1}.png")

            # Call the helper function to save the plot
            visualize_and_save(
                processed_img=images[j],
                gt_mask=gt_masks_np[j],
                pred_raw=preds_before_np[j],
                pred_post=preds_after_np[j],
                save_path=save_path,
                title=f"Test Set - Prediction {image_idx+1}"
            )

# --- 6.6: Print Final Test Results ---
avg_dice_before_test = total_dice_before / num_samples_test
avg_dice_after_test = total_dice_after / num_samples_test
print(f"\n--- Test Set Evaluation Complete ---")
print(f"Total Test Images Processed: {num_samples_test}")
print(f"Average Dice (Before Post-Processing): {avg_dice_before_test:.4f}")
print(f"Average Dice (After Post-Processing):  {avg_dice_after_test:.4f}")

# --- 6.7: Run Prediction on the ENTIRE Train Set & Save ALL Predictions ---
print("\n--- Evaluating ENTIRE Train Set and Saving All Predictions ---")
num_samples_train = 0
total_dice_before_train = 0
total_dice_after_train = 0

with torch.no_grad():
    for i, (images, gt_masks) in enumerate(tqdm(train_loader, desc="Training Set Prediction")):
        images = images.to(DEVICE)
        gt_masks_np = gt_masks.cpu().numpy()

        preds_logits = eval_model(images)
        preds_sigmoid = torch.sigmoid(preds_logits)
        preds_before_np = (preds_sigmoid > 0.5).cpu().numpy()
        preds_after_np = np.array([post_process_mask(np.squeeze(p)) for p in preds_before_np])

        # Loop through each image in the batch to save it
        for j in range(images.shape[0]):
            image_idx = i * train_loader.batch_size + j

            gt = np.squeeze(gt_masks_np[j]).flatten()
            pred_before = np.squeeze(preds_before_np[j]).flatten()
            pred_after = np.squeeze(preds_after_np[j]).flatten()

            intersection_before = (pred_before * gt).sum()
            total_dice_before_train += (2. * intersection_before + smooth) / (pred_before.sum() + gt.sum() + smooth)

            intersection_after = (pred_after * gt).sum()
            total_dice_after_train += (2. * intersection_after + smooth) / (pred_after.sum() + gt.sum() + smooth)

            num_samples_train += 1

            # Define the save path for this specific image
            save_path = os.path.join(TRAIN_SAVE_DIR, f"train_prediction_{image_idx+1}.png")

            # Call the helper function to save the plot
            visualize_and_save(
                processed_img=images[j],
                gt_mask=gt_masks_np[j],
                pred_raw=preds_before_np[j],
                pred_post=preds_after_np[j],
                save_path=save_path,
                title=f"Train Set - Prediction {image_idx+1}"
            )

# --- 6.8: Print Final Train Results ---
avg_dice_before_train = total_dice_before_train / num_samples_train
avg_dice_after_train = total_dice_after_train / num_samples_train
print(f"\n--- Train Set Evaluation Complete ---")
print(f"Total Train Images Processed: {num_samples_train}")
print(f"Average Dice (Before Post-Processing): {avg_dice_before_train:.4f}")
print(f"Average Dice (After Post-Processing):  {avg_dice_after_train:.4f}")
print(f"All predictions saved successfully to Google Drive.")